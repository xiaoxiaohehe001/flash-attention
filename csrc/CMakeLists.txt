cmake_minimum_required(VERSION 3.9 FATAL_ERROR)
project(flash-attention LANGUAGES CXX CUDA)
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)


find_package(Git QUIET REQUIRED)

execute_process(COMMAND ${GIT_EXECUTABLE} submodule update --init --recursive
                WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}
                RESULT_VARIABLE GIT_SUBMOD_RESULT)

#cmake -DWITH_ADVANCED=ON
if (WITH_ADVANCED)
  add_compile_definitions(PADDLE_WITH_ADVANCED)
endif()

add_definitions("-DFLASH_ATTN_WITH_TORCH=0")

set(CUTLASS_3_DIR ${CMAKE_CURRENT_SOURCE_DIR}/cutlass)
set(BINARY_DIR ${CMAKE_BINARY_DIR})

set(FA2_SOURCES_CU
     flash_attn/src/cuda_utils.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim32_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim32_bf16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim64_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim64_bf16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim96_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim96_bf16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim128_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim128_bf16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim160_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim160_bf16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim192_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim192_bf16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim224_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim224_bf16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim256_fp16_sm80.cu
     flash_attn/src/calc_reduced_attn_scores_dispatch/hdim256_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu
     flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu
     flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim160_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim160_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim192_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim224_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim224_bf16_sm80.cu
     flash_attn/src/flash_bwd_hdim256_fp16_sm80.cu
     flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu
  )

add_library(flashattn SHARED
    capi/flash_attn.cu
    ${FA2_SOURCES_CU}
  )
target_include_directories(flashattn PRIVATE
    flash_attn
    ${CUTLASS_3_DIR}/include)

if (WITH_ADVANCED)
set(FA1_SOURCES_CU
    flash_attn_with_bias_and_mask/flash_attn_with_bias_mask.cu
    flash_attn_with_bias_and_mask/src/cuda_utils.cu
    flash_attn_with_bias_and_mask/src/fmha_fwd_with_mask_bias_hdim32.cu
    flash_attn_with_bias_and_mask/src/fmha_fwd_with_mask_bias_hdim64.cu
    flash_attn_with_bias_and_mask/src/fmha_fwd_with_mask_bias_hdim128.cu
    flash_attn_with_bias_and_mask/src/fmha_bwd_with_mask_bias_hdim32.cu
    flash_attn_with_bias_and_mask/src/fmha_bwd_with_mask_bias_hdim64.cu
    flash_attn_with_bias_and_mask/src/fmha_bwd_with_mask_bias_hdim128.cu
    flash_attn_with_bias_and_mask/src/utils.cu)
else()
set(FA1_SOURCES_CU
    flash_attn_with_bias_and_mask/flash_attn_with_bias_mask.cu
    flash_attn_with_bias_and_mask/src/cuda_utils.cu
    flash_attn_with_bias_and_mask/src/utils.cu)
endif()

add_library(flashattn_with_bias_mask STATIC
    flash_attn_with_bias_and_mask/
    ${FA1_SOURCES_CU}
  )

target_include_directories(flashattn_with_bias_mask PRIVATE
    flash_attn_with_bias_and_mask/src
    flash_attn_with_bias_and_mask/cutlass/include
    ${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES})

target_include_directories(flashattn_with_bias_mask INTERFACE
    flash_attn_with_bias_and_mask)

target_link_libraries(flashattn flashattn_with_bias_mask)
  
add_dependencies(flashattn flashattn_with_bias_mask)

set(NVCC_ARCH_BIN 80 CACHE STRING "CUDA architectures")

message("NVCC_ARCH_BIN is set to: ${NVCC_ARCH_BIN}")

STRING(REPLACE "-" ";" FA_NVCC_ARCH_BIN ${NVCC_ARCH_BIN})

set(FA_GENCODE_OPTION "SHELL:")

foreach(arch ${FA_NVCC_ARCH_BIN})
   if(${arch} GREATER_EQUAL 80)
     set(FA_GENCODE_OPTION "${FA_GENCODE_OPTION} -gencode arch=compute_${arch},code=sm_${arch}")
   endif()
endforeach()

target_compile_options(flashattn PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:
  -w
  -Xcompiler="-fPIC"
  -Xcompiler="-O3"
  -std=c++17
  -U__CUDA_NO_HALF_OPERATORS__
  -U__CUDA_NO_HALF_CONVERSIONS__
  -U__CUDA_NO_HALF2_OPERATORS__
  -U__CUDA_NO_BFLOAT16_CONVERSIONS__
  --expt-relaxed-constexpr
  --expt-extended-lambda
  --use_fast_math
  "${FA_GENCODE_OPTION}"
  >)

target_compile_options(flashattn_with_bias_mask PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:
  -w
  -Xcompiler="-fPIC"
  -Xcompiler="-O3"
  -std=c++17
  -U__CUDA_NO_HALF_OPERATORS__
  -U__CUDA_NO_HALF_CONVERSIONS__
  -U__CUDA_NO_HALF2_OPERATORS__
  -U__CUDA_NO_BFLOAT16_CONVERSIONS__
  --expt-relaxed-constexpr
  --expt-extended-lambda
  --use_fast_math
  "${FA_GENCODE_OPTION}"
  >)


INSTALL(TARGETS flashattn
        LIBRARY DESTINATION "lib")

INSTALL(FILES capi/flash_attn.h DESTINATION "include")

if (WITH_ADVANCED)
  if(WIN32)
    set(target_output_name "flashattn")
  else()
    set(target_output_name "libflashattn")
  endif()
  set_target_properties(flashattn PROPERTIES
    OUTPUT_NAME ${target_output_name}_advanced
    PREFIX ""
  )

  configure_file(${CMAKE_SOURCE_DIR}/env_dict.py.in ${CMAKE_SOURCE_DIR}/env_dict.py @ONLY)
  set_target_properties(flashattn PROPERTIES
      LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/paddle_flash_attn/
  )
  add_custom_target(build_whl
      COMMAND ${CMAKE_COMMAND} -E env python ${CMAKE_SOURCE_DIR}/setup.py bdist_wheel
      WORKING_DIRECTORY ${CMAKE_BINARY_DIR}
      DEPENDS flashattn
      COMMENT "Running build wheel"
  )
  
  add_custom_target(default_target DEPENDS build_whl)
  
  set_property(DIRECTORY PROPERTY DEFAULT_TARGET default_target)
endif()
